Now consider Hilbert space $(H, \langle \cdot,\cdot \rangle)$ with the norm defined as $\norm{\cdot}=\sqrt{\langle\cdot,\cdot\rangle}$.

Let be $J: H\rightarrow \mathbb{R}$ a functional over a Hilbert space $H$, we define the set,
\[
{\displaystyle {\underset {v\in C\subseteq H}{\operatorname {arg\,min} }}\,J(x):=\{x\mid x\in H \wedge \forall v\in C:\,J(x)\leq J(v)\}.}\]


By Riesz-Fr\'echet representation formula, exists a unique vector $\nabla J(x) \in H$ such that, 
\[
(\forall y \in H) \quad J'(x; y)=\langle y, \nabla J(x)\rangle
\]
namely Gate\^aux gradient of $J$ at $x$. 

\begin{lemma}
\label{lemma3. Projection}
Let $H$ Hilbert space and $C\subset H$ closed and convex. Define $P_C: H\rightarrow C$, \[P_C(x)=\argmin{v\in C}{\norm{v-x}}.\]
Then,
\begin{enumerate}
	\item $P_C$ is well defined, i.e.  $\forall x \in H $, $\exists !u \in C$ such that $P_C(x)=\{u\}$.
	\item $\forall u,x\in H$, we have $u=P_C(x) \iff u\in C$  and  $\langle x-u, v-u\rangle\leq 0\ \forall v \in C$.
	\item $\forall x,y \in H$, $\langle y-x, P_C(y)-P_C(x)\rangle \geq 0$.
	\item The projection $P_C$ is non expansive. That is $\norm{P_C(x)-P_C(y)}\leq \norm{x-y}$, $\forall x,y\in H$, 
	\item Let be $t>0$ a real number, then $\forall u \in C$, and $\forall v\in H$, $\phi(t)=\frac{1}{t}\norm{P_C\left(u+tv\right)-u}$ is non-increasing.
\end{enumerate}
\begin{proof}\
	\begin{enumerate}
		\item First we prove existence, let be $(v_k)_k$ a minimizing sequence in $C$, such that
		\[
			\norm{x-v_k}\rightarrow \alpha=\inf_{v \in C}\norm{x-v},
		\]
		By the parallelogram law,
		\begin{align*}
			&2\norm{v_j-x}^2+2\norm{v_i-x}^2= \norm{v_j-v_i}^2+\norm{v_j+v_i-2x}^2 \\
			&2\norm{v_j-x}^2+2\norm{v_i-x}^2= \norm{v_j-v_i}^2+4\norm{\frac{v_j+v_i}{2}-x}^2\\
	\implies&2\norm{v_j-x}^2+2\norm{v_i-x}^2-4\norm{\frac{v_j+v_i}{2}-x}^2=\norm{v_j-v_i}^2
		\end{align*}
		Since $C$ is convex $\frac{v_i+v_j}{2} \in C$, then by definition of $\alpha$,
		\[
		0\leq \alpha \leq \norm{\frac{v_j+v_i}{2}-x}
		\]  
		Therefore the above equations become in the following inequality,
		\[
		2\norm{v_j-x}^2+2\norm{v_i-x}^2-4\alpha^2\geq \norm{v_j-v_i}^2
		\]
		
		Since $\norm{v_i-x}\rightarrow \alpha$ and $\norm{v_j -x}\rightarrow \alpha$, we have that $\norm{v_j-v_i}\rightarrow 0$ , therefore the series is Cauchy and then converges. Since $C$ is closed the series converges to a point $v \in C$. 
		
		Second we prove uniqueness, we proceed by contradiction, take $v, v' \in C$ such $v\neq v'$, and both of them minimizing the distant with respect the point $x$, i.e.
		\[
		\norm{x-v}=\norm{x-v'}=\alpha=\min_{u\in C}\norm{u-x}
		\]
		By the parallelogram law,
		\begin{equation*}
			2\norm{x-v}^2+2\norm{x-v'}^2=\norm{2x-v-v'}^2 +\norm{v-v'}^2
		\end{equation*}
		Since $C$ is convex, $\norm{\frac{v+v'}{2}-x}\geq\alpha$
		\begin{align*}
			\norm{v-v'}^2=2\norm{x-v}^2+2\norm{x-v'}^2-\norm{2x-v-v'}^2 \\
			\norm{v-v'}^2=2\norm{x-v}^2+2\norm{x-v'}^2-4\norm{x-\frac{v-v'}{2}}^2 \\
			\norm{v-v'}^2=2\alpha^2+2\alpha^2-4\norm{x-\frac{v-v'}{2}}^2\leq 0
		\end{align*}
		Therefore $\norm{v-v'}=0$, and $v=v'$.
		
		By the uniqueness and existence $\argmin{u\in C}{\norm{u-x}}$ is not empty set and has only one element for each $x \in H$. Thus, $P_C$ is well defined.
		\item 
		First consider that $u \in C$ and $\langle x-u, v-u\rangle \leq 0$, for all $v \in C$. Then,
		\begin{align*}
			\langle x- u, v-u\rangle  & =\langle x- u, x-x+v-u\rangle \\
			& =\langle x- u, x-u\rangle+\langle x- u, v-x\rangle\\
			& =\norm {x- u}^2+\langle x- u, v-x\rangle \\&\leq 0 
		\end{align*}
		Therefore, applying Cauchy-Schwartz inequality we have
		\begin{align*}
		\norm {x- u}^2 &\leq \langle x- u, x-v\rangle \\
		 &\leq \norm{x-u}\norm{x-v} \\
		 \implies \norm {x- u}&\leq \norm{x-v}
		\end{align*}
		And the above holds for every $v \in C$, therefore $u = P_C(x)$. Now consider $u=P_C(x)$, since $P_C(x)$ is well defined by definition $u\in C$. 
		Since $C$ is convex for all $v \in C$, $u_t = u+t(v-u) \in C$, for all $t\in[0,1]$. Since $u=P_C(x)$ and $u_t \in C$, we have $\norm{x-u}^2\leq \norm{x-u_t}^2$
		\begin{align*}
			 \norm{x-u}^2& \leq \langle x-u_t, x-u_t\rangle \\
			 & = \langle x-u-t(v-u),
			 x-u-t(v-u)\rangle \\
 			& = \langle x-u, x-u\rangle -2\langle x-u,
 			t(v-u)\rangle +t^2\langle v-u, v-u\rangle \\
 			0 &\leq t^2\langle v-u, v-u\rangle-2t\langle x-u, v-u\rangle\\
 			0 &\leq t^2\norm{v-u}^2-2t\langle x-u, v-u\rangle
 		\end{align*}
 		For $t = 0$ the above inequality holds (since $u_t=u$), and for all $t>0$, we have 
 		\begin{align*}
	 		\langle x-u, v-u\rangle &\leq \frac{t}{2}\norm{v-u}^2.
		\end{align*}
		Taking $t\downarrow 0$,
 		\begin{align*}
		 		\langle x-u, v-u\rangle &\leq \inf_{t\in(0,1)}\frac{t}{2}\norm{v-u}^2 \\
		 		\langle x-u, v-u\rangle &\leq 0
		\end{align*}
		\item By the above result, we have that $P_C(x) \in C$ and $P_C(y)\in C$, and
		\begin{align*}
			\begin{array}{ccc}
			\langle x-P_C(x), P_C(y)-P_C(x)\rangle \leq 0 & \text{and}
			&\langle y-P_C(y), P_C(x)-P_C(y)\rangle \leq 0
			\end{array}			
		\end{align*}
		Rearranging and adding both inequalities we have,
		\begin{align*}
		\langle x-P_C(x), P_C(y)-P_C(x)\rangle &\leq 
		\langle y-P_C(y), P_C(y)-P_C(x)\rangle \\
		\langle x, P_C(y)-P_C(x)\rangle -\langle P_C(x), P_C(y)-P_C(x)\rangle&\leq \langle y, P_C(y)-P_C(x)\rangle -\langle P_C(y), P_C(y)-P_C(x)\rangle \\
		\langle P_C(y), P_C(y)-P_C(x)\rangle -\langle P_C(x), P_C(y)-P_C(x)\rangle&\leq \langle y, P_C(y)-P_C(x)\rangle -\langle x, P_C(y)-P_C(x)\rangle \\
		\norm{P_C(y)-P_C(x)}^2&\leq \langle y-x, P_C(y)-P_C(x)\rangle.
		\end{align*}
		The above inequality immediately implies $0\leq \langle y-x, P_C(y)-P_C(x)\rangle$.
		\item Using, Cauchy-Schwartz inequality we obtain,
		\begin{align*}
		\norm{P_C(y)-P_C(x)}^2\leq \langle y-x, P_C(y)-P_C(x)\rangle\leq \norm{y-x} \norm{P_C(y)-P_C(x)}.
		\end{align*}
		Hence $\norm{P_C(y)-P_C(x)}\leq \norm{y-x}$, for all $x,y \in H$.
		\item Let $v$ an element of $H$, and $\forall u \in C$, we have $u=P_C(u)$, therefore by the above result,
		\[
			\phi(t)=\frac{1}{t}\norm{P_C(u+tv)-u}=\frac{1}{t}\norm{P_C(u+tv)-P_C(u)} \leq \norm{v}
		\]
		
	\end{enumerate}
\end{proof}
\end{lemma}
\begin{theorem}
Let $H$ be Hilbert space, $C \subset H$ closed and convex, $J: C \rightarrow \mathbb{R}$, Gate\^aux differentiable at the local solution $\overline{u}$ of $\min_{u\in C} J(u)$. Thus, $J'(\overline{u};u-\overline{u}) \geq 0$, $\forall u \in C$ and it is equivalent to $ \overline{u}=P_C(\overline{u}-\delta\nabla J(\overline{u}))$, $\forall \delta >0$. 
\begin{proof} 
Since every Hilbert Space is a Banach space, and $C$ is closed and Convex subset of $H$, and $\overline{u}$ is a solution of minimization problem; we can apply \ref{theorem2. Banach Derivatives}.\\
 Thus	$J'\left(\overline{u}; u-\overline{u}\right)\geq 0 \iff \langle u-\overline{u}, \nabla J(\overline{u})\rangle \geq 0 \ \forall u \in C$. \\
For all $\delta >0$, we multiply the Gate\^aux gradient  $(-\delta)$ and we have,
\[\langle u-\overline{u}, -\delta \nabla J(\overline{u}) \rangle \leq 0 \ \forall u \in C,\] adding zero to the gradient, $ \langle u-\overline{u}, \overline{u}-\delta\nabla J(\overline{u})-\overline{u}\rangle \leq 0$. Then we set $w \in H$ as $w:= \overline{u}-\delta  \nabla J(\overline{u})$, and applying lemma \ref{lemma3. Projection} we have,
\[
	\overline{u}=P_C(w)	\iff \langle u -\overline{u},w-\overline{u}\rangle
\]
Thus, 
	\[
		\overline{u}=P_C(\overline{u}-\delta \nabla J(\overline{u}))
	\]
\end{proof}
\end{theorem}
\subsection{Application}
Consider $U, Y, Z$ Hilbert spaces. Let be  $J: Y\times U \rightarrow \mathbb{R}$ a functional. Consider the minimization problem,
\begin{equation*}
	\left\lbrace
	\begin{array}{l}
	\overline{u}=\underset{y,u}{\min} J(y,u) \\
	Ay=Bu \quad u \in U_{ad} \subset U
	\end{array}
	\right.
\end{equation*}
For some set $U_{ad}$ closed, convex and bounded. And $A \in \mathcal{B}(Y,Z)$ bounded and invertible with $A^{-1}\in \mathcal{B}(Z, Y)$ and $B\in \mathcal{B}(U, Z)$.

Then we can write $y \in Y$ as a function of $u \in U$,
\[
y=y(u)=A^{-1}Bu.
\]
Where $A^{-1}B \in \mathcal{B}(U, Y)$, and adjoint with respect to the inner product $(A^{-1}B)^\star$. Consider the reduced cost functional  $F:U\rightarrow \mathbb{R}$, such that $u\mapsto J(y(u), u)$, then our problem is equivalent to
\[
	\overline{u}_{ad}=\min_{u \in U_{ad}} F(u) 
\]

Let $(u_k)_k \in U_{ad}$ denote a minimizing sequence, i.e. $F(u_k) \rightarrow \inf_{u\in U_{ad}}F(u)$, since $u_k \in U_{ad}$ the sequence is bounded. Therefore we can find a convergent subsequence $u_{k_{l}} \xrightharpoonup[l\rightarrow \infty]{} \overline{u}$, moreover since $U_{ad}$ is closed and convex $U_{ad}$ is weakly closed, implying $\overline{u} \in U_{ad}$

\begin{proposition}
	If $J$ is continuous and weakly lower semicontinuous, then 	$\overline{u}=\argmin{u\in U_{ad}}{F(u)}$.
	\begin{proof}
		If J is weakly lower semicontinuos 
			\[
				J(y(\overline{u}), \overline{u})\leq \liminf_{l\rightarrow \infty} J(y(u_k),u_k)
			\]
		That is, 
		\[
			F(\overline{u}) \leq \liminf_{l\rightarrow \infty} F(u_k) =\alpha 
		\]

	
	Since $u_{k_{l}} \xrightharpoonup[l\rightarrow \infty]{} \overline{u}$, $\implies y(u_k) \xrightharpoonup[]{} y(\overline{u})=\overline{y}$, and $A^{-1}B u_k \xrightharpoonup[]{}A^{-1}B  \overline{u}$
	\end{proof}
\end{proposition}
	If $J$ is Gate\^aux differentiable, applying the chain rule (fact \ref{fact1. Chain Rule}) to $F$ and valuating in $u$ we have
	\[
		F_u(u)=\left.J_y(y,u)\right|_{y=y(u)}\circ y_u(u)+\left.J_u(y,u)\right|_{y=y(u)},
	\]
	where $F_u(u) \in U^*$, $J_y(y,u) \in Y^*$, $y_u(u) \in \mathcal{B}(U, Y)$ and $J_u \in U^*$. Since $U$ and $Y$ are Hilbert spaces, there is a member of each space respectively representing this operator in the inner product. We take $\nabla_u F(u) \in U$,  $\nabla_y J(u) \in Y$, and $\nabla_u J(y, u) \in U$. Therefore we can write $F_u(u;u-\overline{u})$ as follows, and we evaluate our optimality condition,
	\begin{align*}
		0 &\leq \langle u-\overline{u}, \nabla_u F(\overline{u}) \rangle_{UU^*} \quad \forall u \in U_{ad}\\
		&= \langle A^{-1}B(u -\overline{u}), 
		\nabla_y J(\overline{y}, \overline{u}) \rangle_{YY^*}+\langle u-\overline{u},  \nabla_u J(\overline{y},\overline{u})\rangle_{UU^*} \\
		&= \langle A^{-1}B(u -\overline{u}), 
		\nabla_y J(\overline{y}, \overline{u}) \rangle_{YY^*}+\langle u-\overline{u},  \nabla_u J(\overline{y},\overline{u})\rangle_{UU^*} \\
		&= \langle u -\overline{u}, 
		(A^{-1}B)^\star\nabla_y J(y, \overline{u}) \rangle_{UU^*}+\langle u-\overline{u},  \nabla_u J(\overline{y},\overline{u})\rangle_{UU^*} \\
		&= \langle u -\overline{u}, 
		(A^{-1}B)^\star\nabla_y J(\overline{y}, \overline{u})+\nabla_u J(\overline{y},\overline{u})\rangle_{UU^*}
	\end{align*}
	Setting $p^*=(A^{-1}B)^\star\nabla_y J(\overline{y}, \overline{u})$. We have that $\overline{u}=P_{U_{ad}}(\overline{u}-\delta(p^*+\nabla_u J(\overline{y}, \overline{u})))$
