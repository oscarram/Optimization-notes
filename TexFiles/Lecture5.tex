\subsection{Subgradients}

\begin{proposition}
\label{prop5. ConvexProperties}
 Let $J : U \rightarrow (-\infty, \infty]$ be proper and convex, let $x \in 
 \dom J$, and let $y \in U$. Then the following hold:
 \begin{enumerate}
 	\item Let $\phi:\mathbb{R}_+\rightarrow (-\infty, \infty]$, such that $\phi(\alpha) := \frac{J(x+\alpha y) -J(x)}{\alpha}$. The function $\phi$ is increasing.
 	\item $J'(x;y)$ exists in $[-\infty, \infty]$ and  \[J'(x;y)=\inf_{\alpha \in \mathbb{R}_+} \frac{J(x+\alpha y)-J(x)}{\alpha}.\]
 	\item $J'(x; y-x)+J(x)\leq J(y)$.
% 	\item $J'(x; \cdot)$ is sublinear and $J'(x; 0)=0$
% 	\item $J'(x; \cdot)$ is proper, convex, and $\dom J'(x; \cdot) = \cone(\dom f - x)$.
 \end{enumerate}
 
 \begin{proof} \
 	\begin{enumerate}
	\item Fix $\alpha$ and $\beta$ in $\mathbb{R}_+$ such that $\alpha < \beta$, and set $\lambda =\alpha/\beta$ and $z=x+\beta y$. If $J(z)=\infty$, then certainly $\phi(\alpha) \leq \phi(\beta) = \infty$. Consider $J(x)<\infty$, since $J$ is convex,
	 \[J (x + \alpha y) = J(\lambda z + (1 - \lambda)x) \leq \lambda J(z) + (1 - \lambda)J(x) = J(x) + \lambda(J(z)- J(x)).\]
	 Then, substituting $\lambda$ and $z$ 
	 \begin{align*}
			 J (x + \alpha y) - J(x) &\leq \frac{\alpha}{\beta} (J(x+\beta y)-J(x))\\
			 \frac{J (x + \alpha y) - J(x)}{\alpha} &\leq \frac{J(x+\beta y)-J(x)}{\beta}\\
			 \phi(\alpha)&\leq\phi(\beta)
	 \end{align*}
	 Hence, if $\alpha < \beta \implies \phi(\alpha) \leq \phi(\beta)$, i.e. $\phi$ is increasing.
	 \item Since $\phi$ is increasing, 
	 \[
		\inf_{a\in\mathbb{R}_+} \phi({\alpha}) = \inf_{a\in\mathbb{R}_+} \frac{J (x + \alpha y) - J(x)}{\alpha} =\lim\limits_{\alpha \downarrow 0} \frac{J (x + \alpha y) - J(x)}{\alpha} = J'(x; y)
	 \]
	 
	 \item If $y \notin \dom J$ the inequality holds. Consider $y \in \dom J$, and we invoke convexity of $J$ for $\alpha \in (0,1)$, 
	 \begin{align*}
		 J((1-\alpha) x+\alpha y) &\leq (1-\alpha)J(x)+\alpha J(y) \\
		 J((x+\alpha (y-x)) &\leq J(x)+\alpha (J(y)-J(x)) \\
		 J((x+\alpha (y-x)) &\leq J(x)+\alpha (J(y)-J(x)) \\
		 \frac{J((x+\alpha (y-x)) - J(x)}{\alpha} &\leq J(y)-J(x)
	 \end{align*}
	 Taking $\alpha \downarrow 0$, we have
	 \begin{equation*}
	 	J'(x;y-x)\leq J(y)-J(x)
	 \end{equation*}
\end{enumerate}
 \end{proof}
\end{proposition}

\begin{remark}
	Let $J: U \rightarrow (-\infty, \infty]$ be convex, let $x\in U$, and suppose
	that $J$ is G\^ateaux differentiable at $x$. The above result is equivalent to  say $\forall y \in U$ $(y - x \mid \nabla J(x))_{UU^*}+J(x) \leq J(y)$.
\end{remark}	

\begin{proposition}
	 Let $J: U \rightarrow (-\infty, \infty]$ be proper. Suppose that $\dom J$
	 is open and convex, and that $J$ is G\^ateaux differentiable on $\dom J$ . Then the
	 following are equivalent:
	 \begin{enumerate}
		 \item $J$ is convex.
		 \item $\forall x,y \in \dom J, \quad \left(x-y\mid \nabla J(x)\right)_{UU^*}\leq Jx)-J(y)$.
		 \item $\forall x,y \in \dom J, \quad 0 \leq \left(x-y\mid \nabla J(x)-J(y)\right)_{UU^*}$, i.e. $
		 \nabla J(x)$ is monotone.
	 \end{enumerate}
	 \begin{proof}\
  Let $x,y \in \dom J$, and $z \in U$. Since $\dom J$ is open, $\exists \epsilon >0$, such that $x+\epsilon(x-y) \in \dom J$ and $y+\epsilon(y-x) \in \dom J$. Set $C=(-\epsilon, 1+\epsilon)$ and the function $\phi: \mathbb{R}\rightarrow(-\infty, \infty)$

		 \[
			 \phi(\alpha) = J(y+\alpha(x-y))+I_C(\alpha).
		 \]
		 
		 Where $I_C$ is the indicator function.  Then $\phi$ is Gate\^aux differentiable on $C$ and $\forall \alpha \in C$,
		 
		 \[
			 \phi'(\alpha)=\left(x-y\mid\nabla J(y+\alpha (x-y))\right)_{UU^*}
		 \]
		 
		\begin{itemize}
		 \item $(1)\implies (2)$. Since $J$ is proper and convex, assumption number (1), by preposition \eqref{prop5. ConvexProperties} part (3), we have that 
		 	 \begin{equation*}
				\forall y \in \dom J \quad (y - x \mid \nabla J(x))_{UU^*}+J(x) \leq J(y)
		 	 \end{equation*}
		 \item $(2)\implies(3)$. Since assumption (2) holds $y, x \in \dom J$, 
		 \begin{gather*} 
		 (y - x \mid \nabla J(x))_{UU^*}+J(x) \leq J(y) \\
		 		\text{and} \\
		 (x - y \mid \nabla J(y))_{UU^*}+J(y) \leq J(x).
		 \end{gather*}
		 Adding both inequalities, we have
		 \begin{equation*}
		 0 \leq	(x - y \mid \nabla J(x)-\nabla J(y))_{UU^*}
		 \end{equation*}
		 \item $(3)\implies (1)$. Take $\alpha$ and $\beta$ in $C$, such that $\alpha<\beta$, and set $y_\alpha = y +\alpha(x-y)$ and $y_\beta= y +\beta(x-y)$. Then the assumption that for all $x,y \in \dom J$, $(x - y \mid \nabla J(x)-\nabla J(y))_{UU^*} \geq 0$ implies that 
		 
		 \begin{align*}
		 0 &\leq (y_\beta - y_\alpha \mid \nabla J(y_\beta)-\nabla J(y_\alpha))_{UU^*}\\
		   &=(\beta (x-y) - \alpha(x-y) \mid \nabla J(y_\beta)-\nabla J(y_\alpha))_{UU^*} \\
	 	   &=(\beta-\alpha) ((x-y) \mid \nabla J(y_\beta)-\nabla J(y_\alpha))_{UU^*} \\
	 	   &= (\beta-\alpha) \left(\phi'(\beta)-\phi'(\alpha)\right)
		 \end{align*} 
		 
		 Therefore we have that, if $\beta > \alpha$ and assumption (3) holds, $\phi'$ is increasing on $C$, by proposition \eqref{prop0. Convex and increasing derivative}, we have that $\phi$ is convex. Therefore,
		 \[
		 J(\alpha y+(1-\alpha)y)=\phi(\alpha) \leq \alpha \phi(1)+(1-\alpha)\phi(0)=\alpha J(x) +(1-\alpha)J(y)
		 \]
		 Hence $J$ is convex.
		\end{itemize}
	 \end{proof}
\label{prop5. First derivative convex.}
\end{proposition}

\begin{theorem}
	\label{th5. Secon derivative convex.}
	 Let $J: U \rightarrow (-\infty, \infty]$ be proper. Suppose that $\dom J$
	 is open and convex, and that $J$ is twice G\^ateaux differentiable on $\dom J$ . Then,
	 \[
		 (\forall x \in \dom J )(\forall y \in \dom J ) \quad (z\mid  \nabla^2 f(x)z)_{UU^*} \geq 0 \iff J \text{ is convex}
	 \]
	 \begin{proof}
	 	First assume $J$ convex. Let $z \in U$. Since $\dom J$ is open, $\exists \alpha >0$ small enough, such that $x+\alpha z \in \dom J$, and by proposition \eqref{prop5. ConvexProperties} part (3), 
	 	\begin{align*}
	 		0 \leq	(z \mid \nabla J(x+\alpha z)-\nabla J(x))_{UU^*} = \frac{1}{\alpha} ((x+\alpha z) -x \mid \nabla J(x+\alpha z)-\nabla J(x))_{UU^*},
	 	\end{align*}
	 	
	 	letting $\alpha \downarrow 0$ we obtain
	 	\[
	 	0 \leq	(z \mid \nabla^2 J(x)z)_{UU^*} = (\mathsf{D}^2 J(x) z)z.
	 	\] 
	 	Now assume that $\phi$ is twice G\^ateaux differentiable on $\dom J$ with $\forall \alpha \in C$, given by
	 	\[
	 	 \phi''(\alpha) = (x-y \mid \nabla^2 J(y+\alpha(x-y))(x-y))_{UU^*}.
	 	\]
	 	Hence $\phi'$ is increasing and by proposition \eqref{prop0. Convex and increasing derivative}, $\phi$ is convex. Therefore $J$ is convex.
	 \end{proof}
\end{theorem}
\begin{definition}
	Let $U$ be a Banach space and let $J:U\rightarrow (-\infty, \infty]$ be a convex and proper function.
	The subdifferential at a point $u \in \dom J$ is a mapping,
	\[
		\partial J : U \rightarrow 2^{U^*}, \qquad \partial J(u):=\lbrace p^* \in U^* \ | \ J(v)\geq J(u)+p^*(v-u),
		\ \forall v \in U\rbrace
	\]
	The elements of $p^* \in \partial J(u)$ are called subgradients of $J$ at $u$.
\end{definition}
\begin{example}
	Consider $J:\mathbb{R}\rightarrow \mathbb{R}$, $u\rightarrow \abs{u}$ which is not differentiable at $u=0$. If $u>0$, then $J(u)=u$ and we can find $0<v<u<w$. Then $p^* \in \partial J(u)$ implies by definition of subdifferential
	\begin{align*}
		v-u \geq p^*(v-u) &\equiv (1-p^*)(u-v) \leq 0 \\
		w-u \geq p^*(w-u) &\equiv (1-p^*)(w-u) \geq 0.
	\end{align*}
	which implies for $u>0$, $p^* \leq 1 \leq p^*$, then $p^*=1$.
	
	In the same way we obtain for $u<0$, $p^*\geq -1 \geq p^*$. 
	In the case $u=0$, we need to satisfy $\abs{v}\geq p^*v$, which is fulfilled if and only if $\abs{p^*}\leq 1$. Hence for $J(u)=\abs{u}$,
	\begin{equation*}
	\partial \abs{u}=
	\left\lbrace
		\begin{array}{lr}
		\{1\}, & u>0 \\
		\left[-1,1\right], & u=0 \\
		\{-1\}, & u < 0
		\end{array}
	\right. .
	\end{equation*}
\end{example}

\begin{example}
	A convex function which is not subdifferentiable everywhere $J:\mathbb{R} \rightarrow \mathbb{R}$, 
	\begin{equation*}
		J(u)=\left\lbrace\begin{array}{cl}
		 -\sqrt{1-\abs{u}^2} &\quad \abs{u}\leq 1 \\
		 \infty &\quad \text{otherwise}
		\end{array}
	\right.
	\end{equation*}
	For $\abs{u}\geq 1$, we have $\partial J(u) = \emptyset$.
\end{example}

\begin{example}
	Let $C$ be a convex and closed subset of $U$ and $I_C$ function defined by
	\[
		I_C (u)=
		\left\lbrace
		\begin{array}{cl}
			0 \quad& u\in C \\
			\infty \quad& \text{otherwise}
		\end{array}
		\right.
	\]
	The subdifferentiable is the definition of normal cone at $u$
	\[\partial I_C (u)=\lbrace u^* \in U^* \ | \ u^*(u-v)\geq \forall v \in C\rbrace = \mathcal{N}_C(u)\].

\end{example}

\begin{theorem}
	Let $U$ be a Banach space. And $J: U\rightarrow \overline{\mathbb{R}}$ a subdifferentiable function. Then $\partial J(u)$ is convex and weakly closed.
\end{theorem}

\begin{remark}
	Most of the rules for derivates also hold for subdifferentials with some additional assumptions,
	\begin{itemize}
		\item $J:U\rightarrow \overline{\mathbb{R}}$, $\lambda > 0$, $\partial J(\lambda u)=\lambda J(u)$.
		\item $\partial(J+F)(u) \supseteq \partial J(u)+ \partial F(u)$.
	\end{itemize}
\end{remark}

\begin{theorem}[Moreau-Rockafellar]
	\label{th5. Moreau-Rockafellar}
	Let $U$ be a Banach space and $J_i: U\rightarrow \mathbb{R}$ proper and convex functions for $i=1,\dots, n$.
	The sum-rule
	\[
		\partial(J_1+\dots+J_n)(u)=\partial J_1(u)\dots \partial J_n(u), \qquad n\geq 2
	\]
	holds if there exists $u_0 \in U$ such that all $J_i(u_0)$ are finite and all $J_i$ except at most one $J_k$, $k\in \lbrace 1,2,\dots n\rbrace$ are continuous at $u_0$
\end{theorem}