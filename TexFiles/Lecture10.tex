\subsection{Minimization}
Two approaches for the solution of infinite-dimensional optimization problems.

\begin{itemize}
	\item \textbf{Discretize then Optimize:} this approach consists of a direct discretization of the problem which leads to a non-linear programming problem in a finite subspace $U_n \subset U$. If the discretization is accurate enough, the approximate solution $\overline{u_n} \in U_n$ is close to the real solution $\overline{u} \in U$
	
	\item \textbf{Optimize then Discretize:} The idea is to formulate the problem in infinite dimensions and to apply discretization only for the solution of sub-problems and for the evluation of the objective function. The main advantage is that quantitative estimates for the convergence of the optimization method can be combined with error estimates of the discretization.
\end{itemize}
The general framework for minimization algorithms for 
\[
	\min_{u\in U} J(u)
\]
with $J:U\rightarrow \overline{\mathbb{R}}$, $U$ Banach space delivers a sequence $(u_k)_k \in U$.

\paragraph{Desired Concepts}
\begin{enumerate}
	\item \textbf{Global convergence: }We need to measure if a limit is a candidate for a solution of \eqref{eq8. P}.
	\begin{definition}[Stationary measure]
		Let $\Sigma: U\rightarrow \mathbb{R}_+$ a functional, given by
		\begin{align*}
		\Sigma(u)=
		\left\lbrace	
		\begin{array}{ll}
			0 & \quad u \text{ is stationary point}.\\
			\alpha(u) \in \mathbb{R}_+ & \quad \text{everywhere else}			
			\end{array}
		\right.
		\end{align*}
	We call this function an stationary measure. 
	\end{definition}
	\begin{example}\
	\begin{itemize}
		\item For unconstrained problems: \[\Sigma(u)=\norm{\nabla J(u)}_{U^*}\]
		\item In the case $U$ is a Hilbert space and the problem restricted over a closed and convex set $C\subset U$. 
		\[
		\Sigma(u)= \norm{u-P_c(u-\nabla J(u))}
		\]
	\end{itemize}
	\end{example}
	We have global convergence if:
	\begin{itemize}
		\item Every accumulation point of $(u_k)_k$ is a stationary point.
		\item $\lim_{k\rightarrow \infty} \Sigma(u_k)=0$ for some continuous stationary measure.
		\item $(u_k)_k$ has an accumulation point which is stationary.
		\item $\liminf_{k\rightarrow\infty} \Sigma(u_k) =0$ and $\Sigma$ continuous.
	\end{itemize}
	\item \textbf{Fast local convergence:}
	Let $\overline{u}$ be a stationary point of $\eqref{eq8. P}$. We say that $u_k \rightarrow \overline{u}$ locally with \textit{q-superlinear rate} if $\norm{u_{k+1}-\overline{u}}_U=C_k\norm{u_k-\overline{u}}_U$, as $k\rightarrow \infty$ in a neighborhood of $\overline{u}$ and $C_k\rightarrow 0$ as $k\rightarrow 0$.
	We have convergence of order $\alpha +1$, $\alpha >0$ if 
	$\norm{u_{k+1}-\overline{u}}_U=C_k\norm{u_k-\overline{u}}^{\alpha+1}_U$. With $\alpha =1$ we obtain \textit{q-quadratic} convergence.
\end{enumerate}


\subsection{Gradient methods}
In the following, we assume $H$ to be Hilbert space.
\paragraph{Motivation}
	Dynamical systems in physics are often based on the idea of gradient flow with respect to the energy which the system follows. Consider for example the heat equation. Hence, the thermal energy is given by
	\[
		E(u)=\frac{1}{2}\int \abs{\nabla u}^2 dx.
	\]
	
	The gradient flow is defined by,
	\[
		\frac{\partial u}{\partial t} = -E(u),
	\]
	which yields the heat equation.
	\[
		\dot{u}(t)=\Delta u
	\]
In order to obtain a minimization method in $U$, we introduce the gradient flow $\frac{\partial u}{\partial t} = -\nabla J(u)$, with $\nabla J(u)\in H$ which can be associated with the gradient of $J$ at $u$, i.e.

\[
	\left\langle  v,\pdev{u}{t} \right\rangle = \left\langle v, \nabla J(u)\right\rangle = -J'(u;v), \quad \forall v \in H
\]

The evolution of $J$ corresponding to the gradient flow is given by

\[
	\frac{\partial}{\partial t} \left(J(u)\right) = \left\langle \pdev{u}{t}, \nabla J(u) \right\rangle = -\norm{\pdev{u}{t}}^2 \leq 0
\]

i.e. the objective function is decreasing and \[\pdev{}{t} J(u) =0 \iff \pdev{u}{t}=0\].

Consequently, the gradient flow will decrease the objective function $J$ until the evolution arrives a stationary point. In order to obtain an iterative optimization method, we use an explicit time discretization of the flow, i.e.
\begin{align*}
u_{k+1}=u_k - \tau_k\nabla J(u_k) &\quad \text{for } k=0,1,2,.. \\
\nabla J(u_{k+1})=0 &\quad \text{Stop the iteration.}
\end{align*}
with appropriate (small) choice of the time step $\tau_k >0$. 
\begin{remark}
We are only interested in the minimization of the objective function and not in the accurate approximation of the solution of the gradient flow.
\end{remark}


Therefore we select the step size purely based on the suitable descent of objective function. A classical way to do this is the so called \textit{Armijo-Goldstein} rules, which are based on the ``effective descent",
\[
\mathcal{D}_{\text{eff}}(\tau) = J(u_k+\tau s)-J(u_k)
\]
and the ``expected descent"
\[
	\mathcal{D}_{\text{exp}} = \tau J'(u_k;s)=\tau \langle s, \nabla J(u_k)\rangle
\]
where  $s=-\nabla J(u_k)$. They are related to each other by the Taylor formula,
\[
	\mathcal{D}_{\text{eff}}(\tau)=\mathcal{D}_{\text{exp}}(\tau)+O(\tau)
\]
Therefore, we can test if 
\begin{equation}
\alpha \mathcal{D}_{\text{exp}}(\tau)\leq \mathcal{D}_{\text{eff}}(\tau) \leq \beta \mathcal{D}_{\text{exp}}(\tau) \label{eq10. DexpDeffDexp} \tag{\EOPatron}
\end{equation}
with constants $0<\beta<\alpha<1$. 

If $\mathcal{D}_{\text{eff}}(\tau) > \beta \mathcal{D}_{\text{exp}}(\tau)$, then $\tau$ is too large and we decrease it. 
If $\mathcal{D}_{\text{eff}}(\tau)<\alpha\mathcal{D}_{\text{exp}}(\tau)$, we could still increase $\tau$. We accept $\tau$, if 
\eqref{eq10. DexpDeffDexp} is fulfilled. Typical choices of the constants are $\alpha \approx 0.9$ and $\beta \approx 0.1$.

Moreover, the strategy for increasing and decreasing $\tau$ should be different, i.e. multiplying with $1.5$, for increasing and dividing by $2$ for decreasing.

\begin{theorem}
	Let $J:H\rightarrow \overline{\mathbb{R}}$ be twice Fr\'echet-differentiable and weakly lower semicontinuous on a Hilbert space $H$. Moreover, let the level sets,
	\[
		\mu_\xi = \{ u \in U \mid J(u)\leq \xi \}
	\]
	be bounded in $H$ for each $\xi \in \mathbb{R}$ and empty for sufficiently small $\xi$. Then the sequence $(u_k)_k$ generated by the gradient method with the \textit{Amijo-Goldstein} line search has a weakly convergent subsequence, whose limit is a stationary point. 
	
	\begin{proof}
		Since the gradient method is a descent method we have $J(u_k) \leq J(u_0)$, $\forall k \geq 0$,
		i.e. $(u_k)_k$ is bounded, the lemma \eqref{lemma0. Bounded and weakly convergent}, implies that $\exists (u_{k_l})_l \xrightharpoonup[l\rightarrow\infty]{} \overline{u}$.
		
		Therefore,
		\begin{align*}
			\sum_{k=0}^N \norm{u_{k+1}-u_k}^2 &= \sum_{k=0}^N \langle u_{k+1}-u_{k}, -\tau_k \nabla J(u_k)\rangle
		\end{align*}
		Considering that,
		\begin{align*}
			\frac{1}{\beta} \mathcal{D}_{\text{eff}}(\tau)=\frac{1}{\beta}\left(J(u_k+\tau s)-J(u_k)\right) \leq \mathcal{D}_{\text{exp}}(\tau)=\tau J'(u_k;s)
		\end{align*}
		Then we have substituting, 
		\begin{align*}
						\sum_{k=0}^N \norm{u_{k+1}-u_k}^2&\leq \frac{1}{\beta} \sum_{k=0}^{N} (J(u_k)-J(u_{k+1})) \\&= \frac{1}{\beta} (J(u_0)-J(u_{N+1})) \\
			&\leq \frac{1}{\beta} (J(u_0)-\inf_{u\in U} J(u)) = q
		\end{align*}
	$q$ is independent of $N$; for $N\rightarrow \infty$ we obtain 
	\[
	\sum_{l=0}^{\infty} \norm{u_{{k_l}+1}-u_{k_l}}^2=\sum_{k=0}^{\infty} \norm{u_{k+1}-u_{k}}^2\leq q
	\]
	
	Thus, there is a subsequence $(u_k)_l$, such that
	\[
	\norm{\tau_{k_l}\nabla J(u_{k_l})}=\norm{u_{{k_l}+1}-u_{k_l}} \xrightarrow{l\rightarrow \infty} 0.
	\]
	
	$\exists c >0$, $J''(u_{k_l};(v,v))\leq c \norm{v}^2$, $\forall v \in U$. 	Amijo-Goldstein implies,
	\begin{align*}
		\alpha \mathcal{D}_{\text{exp}} (\tau) &= \alpha J'(u_{k_l};u_{{k_l}+1}-u_{{k_l}})\\ &\leq \alpha \mathcal{D}_{\text{exp}}(\tau_{k_l})\\&= J(u_{k_l+1})-J(u_{k_l}) \\
		&= J'(u_{k_l};u_{k_l+1}-u_{k_l}) + \int_{0}^{1} J''\left(u_{k_l}+t(u_{k_l+1}-u_{k_l}); \left((u_{k_l+1}-u_{k_l}, u_{k_l+1}-u_{k_l})\right)\right) dt \\
		& \leq J'(u_{k_l};u_{k_l+1}-u_{k_l}) + C \norm{u_{k_l+1}-u_{k_l}}^2
		\end{align*}
	implies,
	\begin{align*}
	(\alpha -1) J'(u_{k_l};u_{k_l+1}-u_{k_l}) &\leq C \norm{u_{k_l+1}-u_{k_l}}^2 
	\end{align*}
	Since $u_{k_l+1}-u_{k_l}= -\tau_k \nabla J(u_{k_l})$.
	\begin{align*}
		(1-\alpha) \tau_{k_l} \norm{\nabla J(u_{k_l})}^2 \leq c \tau_{k_l}^2\norm{\nabla J(u_{k_l})}^2
	\end{align*}
	
	Therefore, $\nabla J(u_{k_l})=0$, or $ 1-\alpha \leq c \tau_{k_l}$. But since $1-\alpha > 0$ and $c <0$. 
	We have $\nabla J(u_{k_l}) =0$, implying the algorithm reached stationary point; 
		\[\forall j \geq k_l \quad u_j= u_{k_l}  \implies \norm{\nabla J(u_{k_l})} \rightarrow 0 \implies \nabla J(\overline{u})=0\].
	\end{proof}
\end{theorem}

\begin{remark}
	The Armijo-Goldstein rule can be applied to any method which yields a descent direction, i.e. an element $s\in U$ of a Banach space satisfying
	\[
		\left(s \mid \nabla J(u_k)\right)_{UU^*} < 0
	\]
	\paragraph{Descent Method:} For $k=0,1,\dots$
		\begin{itemize}
			\item Choose descent direction $s^k \in U: (s^k \mid \nabla J(u_k))_{UU^*}<0$.
			\item Choose step size $\tau_k >0 : J(u_k+\tau_k s^k) < J(u_k)$.
			\item $u_{k+1}=u_k+\tau_k s^k$.
		Stop iteration if $\nabla J(u_{k+1})=0$.
		\end{itemize}
\end{remark}
